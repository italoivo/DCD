import numpy as np
from multiprocessing import Pool
import dynamic_louvain
import scipy.stats as stats
import math
import numpy.matlib

def comm_time_calc(matrix):
    """
    Calculates the distribution of time nodes take to change community.
    Parameters
    ----------
    matrix : Adjacency matrix.
    Returns
    -------
    comm_times : Distribution of time a node takes to change community.
    """
    time_len,nodes = matrix.shape
    comm_times = []
    for i in range(nodes):
        continuity_sequence = matrix[1:,i]-matrix[:-1,i]==0
        length_series = []
        length = 1
        for i in continuity_sequence:
            if i:
                length += 1
            else:
                length_series.append(length)
                length = 1
        length_series.append(length)
        comm_times.extend(length_series)
    return comm_times

def cumulative_distances_calc(data,lower_threshold):
    """
    Calculates the distance between the curves of cumulative distributions of
    the data and the values generated by a power-law model with exponent
    calculated by the maximum likelihood estimator method.
    ----------
    data : Distribution of values to fit the model.
    lower_threshold : minimum value to consider for the comparison
    Returns
    -------
    ks_distances : Mean of the distances between the curves, the mean is
    calculated over the existing bins.
    """
    comm_sizes = data
    comm_sizes = np.array(comm_sizes)
    comm_sizes = comm_sizes[comm_sizes>=lower_threshold]
    
    a,_ = mle(comm_sizes,lower_threshold)
    
    ks_sample = comm_sizes
    bins = np.arange(np.amin(comm_sizes),np.amax(comm_sizes))
    
    empirical,_ = np.histogram(comm_sizes,bins=bins,density=True)
    empirical = np.cumsum(empirical)
    ks_distances = []
    for _ in range(100):
        synt_data = synt_data_generator(a,ks_sample,lower_threshold)
        synthetic,_ = np.histogram(synt_data,bins=bins,density=True)
        synthetic = np.cumsum(synthetic)
        ks_distances.append(np.mean(np.abs(empirical-synthetic)))
    ks_distances = np.mean(ks_distances)
    return ks_distances

def mle(x, xmin=None, xmax=None):
    """
    Calculates the exponent of a power law using the maximum likelihood 
    estimator method.
    ----------
    x : Distribution of values to fit the model.
    xmin : minimum value to consider for fitting the power law
    xmax : maximum value to consider for fitting the power law
    Returns
    -------
    tau : Estimated exponent calculated.
    Ln : Log-likelihood for the data to be sampled from the model obtained.
    """
    if (xmin==None):
        xmin = np.min(x)
    if (xmax==None):
        xmax = np.max(x)
    tauRange=np.array([1.000001,5])
    precision = 10**(-3)
    # Error check the precision
    if math.log10(precision) != round(math.log10(precision)):
        print('The precision must be a power of ten.')

    x = np.reshape(x, len(x))

    #Determine data type
    if np.count_nonzero(np.absolute(x - np.round(x)) > 3*(np.finfo(float).eps)) > 0:
        dataType = 'CONT'
    else:
        dataType = 'INTS'
        x = np.round(x)

    # print(dataType)
    #Truncate
    z = x[(x>=xmin) & (x<=xmax)]
    unqZ = np.unique(z)
    nZ = len(z)
    nUnqZ = len(unqZ)
    allZ = np.arange(xmin,xmax+1)
    nallZ = len(allZ)

    #MLE calculation

    r = xmin / xmax
    nIterations = int(-math.log10(precision))

    for iIteration in range(1, nIterations+1):

        spacing = 10**(-iIteration)

        if iIteration == 1:
            taus = np.arange(tauRange[0], tauRange[1]+spacing, spacing)

        else: 
            if tauIdx == 0:
                taus = np.arange(taus[0], taus[1]+spacing, spacing)
            elif tauIdx == len(taus):    
                taus = np.arange(taus[-2], taus[-1]+spacing, spacing)#####
            else:
                taus = np.arange(taus[tauIdx-1], taus[tauIdx+1]+spacing, spacing)

        #return(dataType)        
        nTaus = len(taus)

        if dataType=='INTS':#this comes from Marshall et. al. 2016.
            #replicate arrays to equal size
            allZMat = np.matlib.repmat(np.reshape(allZ,(nallZ,1)),1,nTaus)
            tauMat = np.matlib.repmat(taus,nallZ,1)

            #compute the log-likelihood function
            #L = - np.log(np.sum(np.power(allZMat,-tauMat),axis=0)) - (taus/nZ) * np.sum(np.log(z))
            L = - nZ*np.log(np.sum(np.power(allZMat,-tauMat),axis=0)) - (taus) * np.sum(np.log(z))

        elif dataType=='CONT':#this comes from Corral and Deluca 2013.
            #return (taus,r, nZ,z)
            L = np.log( (taus - 1) / (1 - r**(taus - 1)) )- taus * (1/nZ) * np.sum(np.log(z)) - (1 - taus) * np.log(xmin)

            if np.in1d(1,taus):
                L[taus == 1] = -np.log(np.log(1/r)) - (1/nZ) * np.sum(np.log(z))
        tauIdx=np.argmax(L)

    tau = taus[tauIdx]
    Ln = L[tauIdx]
    #it returns the exponent and the log-likelihood.
    return (tau, Ln)

def synt_data_generator(a,comm_sizes,lower_threshold):
    """
    Generates a synthetic series of values using the power law model
    calculated using MLE.
    ----------
    a : power law exponent
    comm_sizes : Distribution of values used to fit the model.
    lower_threshold : minimum value used for fitting the power law
    Returns
    -------
    synt_data : synthetic data obtained from the model within the same range
    as the original data.
    """
    comm_sizes = np.array(comm_sizes)
    comm_sizes = comm_sizes[comm_sizes>=lower_threshold]
    synt_data = np.random.zipf(a, len(comm_sizes))

    synt_data = synt_data[synt_data>=lower_threshold]
    synt_data = synt_data[synt_data<=np.amax(comm_sizes)]
    size_diff = len(comm_sizes) - len(synt_data)
    while size_diff > 0:
        add_data = np.random.zipf(a, size_diff)
        synt_data = np.append(synt_data,add_data)
        synt_data = synt_data[synt_data>=lower_threshold]
        synt_data = synt_data[synt_data<=np.amax(comm_sizes)]
        size_diff = len(comm_sizes) - len(synt_data)
    return synt_data

def comm_size_calc(matrix):
    """
    Calculates the community size distribution for all time layers.
    ----------
    matrix : dynamic community structure.
    Returns
    -------
    comm_sizes : list with the community sizes in the dynamic community
    structure.
    """
    no_tl,_ = matrix.shape
    comm_sizes = []
    for tl in range(no_tl):
        comm_labels = np.unique(matrix[tl,:])
        for label in comm_labels:
            comm_size = np.sum(matrix[tl,:]==label)
            comm_sizes.append(comm_size)
    return comm_sizes

def parameter_search(matrices,lower_gamma,higher_gamma,lower_omega,higher_omega,iterations):
    """
    Searches for the gamma and omega parameters that minimizes the spatial
    skewness and maximizes the scalefreeness for the adjacency matrices used.
    ----------
    matrices : adjacency matrices to use to obtain the community structures.
    lower_gamma : minimum value of gamma to consider in the parameter search.
    higher_gamma : maximum value of gamma to consider in the parameter search.
    lower_omega : minimum value of omega to consider in the parameter search.
    higher_omega : maximum value of omega to consider in the parameter search.
    iterations : number of iterations to perform.
    Returns
    -------
    gamma_omega_sequence : values of optimized gamma and omega for each 
    iteration.
    step_comm_sizes : values of gamma and absolute value of skewness for the 
    values used in each iteration
    step_durations : values of omega and mean distance between the cumulative
    distributions of the model and the data for each iteration.
    """
    step_comm_sizes = []
    current_omega = 0.0
    step_durations = []
    gamma_omega_sequence = []

    for _ in range(iterations):
        output_samples = []
        for adj_matrix in matrices:
            _,mat_size,_ = adj_matrix.shape
            gamma_values=np.linspace(lower_gamma,higher_gamma,10)
            parameters = [(adj_matrix, i, current_omega) for i in gamma_values]
    
            num_processors = 10
            p=Pool(processes = num_processors)
            output_samples = []
            output = p.starmap(dynamic_louvain.dynamic_louvain,parameters)
            output_samples.append(output)
    
        comm_sizes_samples = []
        for output in output_samples:
            comm_sizes_stats = []
            for entry in output:
                comm_structure = entry[2]
                comm_sizes = comm_size_calc(comm_structure)
                if np.mean(comm_sizes)>1 and np.mean(comm_sizes)<mat_size:
                    comm_sizes_stats.append([entry[0],np.abs(stats.skew(comm_sizes))])
            comm_sizes_samples.append(comm_sizes_stats)
        comm_sizes_samples = np.array(comm_sizes_samples)
    
        mean_comm_sizes = np.mean(comm_sizes_samples,axis=0)
        step_comm_sizes.append(np.transpose(mean_comm_sizes))

        minima_arg = np.argmin(mean_comm_sizes[:,1])
        current_gamma = mean_comm_sizes[minima_arg,0]
        step_size = mean_comm_sizes[1,0] - mean_comm_sizes[0,0]
        lower_gamma = current_gamma - step_size
        higher_gamma = current_gamma + step_size
    
        output_samples = []
        for adj_matrix in matrices:
            omega_values=np.linspace(lower_omega,higher_omega,10)
            parameters = [(adj_matrix, current_gamma, i) for i in omega_values]
    
            num_processors = 10
            p=Pool(processes = num_processors)
            output = p.starmap(dynamic_louvain.dynamic_louvain,parameters)
            output_samples.append(output)
        
        duration_distributions = []
        for sample in output_samples:
            sample_durations = []
            for entry in sample:
                comm_structure = entry[2]
                comm_sizes = comm_time_calc(comm_structure)
                sample_durations.append(comm_sizes)
            duration_distributions.append(sample_durations)
    
        concat_samples = [[] for i in omega_values]
        for index in range(len(omega_values)):
            for sample in duration_distributions:
                concat_samples[index].extend(sample[index])
            
        ks_distances = []
        lower_threshold = 2
        for entry in concat_samples:
            comm_sizes = entry
            ks_distances.append(cumulative_distances_calc(entry,lower_threshold))
        ks_distances = np.array(ks_distances)
    
        step_durations.append(np.array([omega_values,ks_distances]))
    
        minima_arg = np.argmin(ks_distances)
        current_omega = omega_values[minima_arg]
        step_size = omega_values[1] - omega_values[0]
        lower_omega = current_omega - step_size
        higher_omega = current_omega + step_size
    
        gamma_omega_sequence.append([current_gamma,current_omega])
    #step_comm_sizes = np.transpose(step_comm_sizes)
    return gamma_omega_sequence,step_comm_sizes,step_durations